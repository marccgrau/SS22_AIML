{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in ./venv/lib/python3.9/site-packages (1.2.10)\r\n",
      "Requirement already satisfied: affine in ./venv/lib/python3.9/site-packages (from rasterio) (2.3.1)\r\n",
      "Requirement already satisfied: click>=4.0 in ./venv/lib/python3.9/site-packages (from rasterio) (8.1.2)\r\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from rasterio) (2021.10.8)\r\n",
      "Requirement already satisfied: attrs in ./venv/lib/python3.9/site-packages (from rasterio) (21.4.0)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.9/site-packages (from rasterio) (1.22.3)\r\n",
      "Requirement already satisfied: snuggs>=1.4.1 in ./venv/lib/python3.9/site-packages (from rasterio) (1.4.7)\r\n",
      "Requirement already satisfied: cligj>=0.5 in ./venv/lib/python3.9/site-packages (from rasterio) (0.7.2)\r\n",
      "Requirement already satisfied: click-plugins in ./venv/lib/python3.9/site-packages (from rasterio) (1.1.1)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from rasterio) (60.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in ./venv/lib/python3.9/site-packages (from snuggs>=1.4.1->rasterio) (3.0.8)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/home/mcgrau/PycharmProjects/SS22_AIML/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: wandb in ./venv/lib/python3.9/site-packages (0.12.14)\r\n",
      "Requirement already satisfied: GitPython>=1.0.0 in ./venv/lib/python3.9/site-packages (from wandb) (3.1.27)\r\n",
      "Requirement already satisfied: protobuf>=3.12.0 in ./venv/lib/python3.9/site-packages (from wandb) (3.20.0)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in ./venv/lib/python3.9/site-packages (from wandb) (8.1.2)\r\n",
      "Requirement already satisfied: promise<3,>=2.0 in ./venv/lib/python3.9/site-packages (from wandb) (2.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./venv/lib/python3.9/site-packages (from wandb) (2.8.2)\r\n",
      "Requirement already satisfied: pathtools in ./venv/lib/python3.9/site-packages (from wandb) (0.1.2)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./venv/lib/python3.9/site-packages (from wandb) (1.5.10)\r\n",
      "Requirement already satisfied: PyYAML in ./venv/lib/python3.9/site-packages (from wandb) (6.0)\r\n",
      "Requirement already satisfied: six>=1.13.0 in ./venv/lib/python3.9/site-packages (from wandb) (1.16.0)\r\n",
      "Requirement already satisfied: setproctitle in ./venv/lib/python3.9/site-packages (from wandb) (1.2.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./venv/lib/python3.9/site-packages (from wandb) (5.9.0)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./venv/lib/python3.9/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./venv/lib/python3.9/site-packages (from wandb) (2.27.1)\r\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in ./venv/lib/python3.9/site-packages (from wandb) (1.0.8)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/home/mcgrau/PycharmProjects/SS22_AIML/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio.plot import reshape_as_image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# import the PyTorch deep learning library\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# create models sub-directory inside the Colab Notebooks directory\n",
    "models_directory = '/home/mcgrau/PycharmProjects/SS22_AIML/models'\n",
    "test_directory = '/home/mcgrau/PycharmProjects/SS22_AIML/data/testset'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"AnnualCrop\",\n",
    "    \"Forest\",\n",
    "    \"HerbaceousVegetation\",\n",
    "    \"Highway\",\n",
    "    \"Industrial\",\n",
    "    \"Pasture\",\n",
    "    \"PermanentCrop\",\n",
    "    \"Residential\",\n",
    "    \"River\",\n",
    "    \"SeaLake\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "27000"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change this to your eurosat path\n",
    "eurosat_dir = \"ds/images/remote_sensing/otherDatasets/sentinel_2/tif\"\n",
    "samples = glob.glob(os.path.join(eurosat_dir, \"*\", \"*.tif\"))\n",
    "len(samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PermanentCrop\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 111\n",
    "sample = samples[sample_idx]\n",
    "label = sample.split('/')[-1].split('_')[0]\n",
    "print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] notebook with cuda computation enabled\n"
     ]
    }
   ],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# init deterministic GPU seed\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled[2, 2, 2, 2]\n",
    "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 27 15:11:23 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   45C    P0    21W /  N/A |   7086MiB /  7982MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1854      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A      3763      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A     30075      C   ...SS22_AIML/venv/bin/python     7073MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
    "class_to_idx = {value:key for key,value in idx_to_class.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [1353.6840, 1116.9715, 1041.5042,  945.7788, 1198.5204, 2002.4349,\n",
    "         2373.4563, 2300.6978,  732.0590, 1818.9174, 1116.6823, 2599.1740],\n",
    "                         std = [ 245.3512,  333.1604,  394.8386,  594.0629,  566.8239,  861.8696,\n",
    "         1088.1853, 1119.6170,  404.3341, 1002.6676,  760.1735, 1233.1597])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    \"\"\"Create own dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        self.eurosat_dir = \"ds/images/remote_sensing/otherDatasets/sentinel_2/tif\"\n",
    "        self.samples = glob.glob(os.path.join(self.eurosat_dir, \"*\", \"*.tif\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        input = self.samples[idx]\n",
    "        label = input.split('/')[-1].split('_')[0]\n",
    "        label = class_to_idx[label]\n",
    "        with rio.open(input, \"r\") as d:\n",
    "          image = d.read([1,2,3,4,5,6,7,8,9,11,12,13]).astype(int)\n",
    "          image = reshape_as_image(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image.astype(float))\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "trainData = SatelliteDataset(transform=transformer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-2.9217e-01, -2.9217e-01, -3.2070e-01,  ..., -1.4544e-01,\n           -1.4136e-01, -1.3729e-01],\n          [-2.9217e-01, -2.9217e-01, -3.2070e-01,  ..., -1.4544e-01,\n           -1.4136e-01, -1.3729e-01],\n          [-2.9217e-01, -2.9217e-01, -3.2070e-01,  ..., -1.3729e-01,\n           -1.3729e-01, -1.3321e-01],\n          ...,\n          [-1.0468e-01, -1.0468e-01, -1.0468e-01,  ..., -6.8636e-03,\n           -4.7622e-02, -8.4304e-02],\n          [-1.0061e-01, -1.0061e-01, -1.0061e-01,  ...,  1.2879e-03,\n           -4.3546e-02, -8.0228e-02],\n          [-9.2455e-02, -9.2455e-02, -9.2455e-02,  ...,  1.7591e-02,\n           -2.7243e-02, -6.3925e-02]],\n \n         [[ 8.4129e-02,  8.4129e-02,  2.0119e-01,  ...,  3.0101e-02,\n            6.3118e-02,  6.3118e-02],\n          [ 8.4129e-02,  8.4129e-02,  2.0119e-01,  ...,  3.0101e-02,\n            6.3118e-02,  6.3118e-02],\n          [ 1.9519e-01,  1.9519e-01,  2.4098e-02,  ...,  6.9121e-02,\n            8.7131e-02,  9.0132e-02],\n          ...,\n          [-2.9160e-03, -2.9160e-03, -1.7924e-02,  ...,  5.4037e-01,\n            8.7131e-02, -2.3704e-01],\n          [ 4.8110e-02,  4.8110e-02,  8.1128e-02,  ...,  5.3136e-01,\n           -1.1698e-01, -2.8506e-01],\n          [ 2.1920e-01,  2.1920e-01,  1.5016e-01,  ...,  2.8223e-01,\n           -1.2898e-01, -2.4304e-01]],\n \n         [[ 5.7871e-01,  5.7871e-01,  5.1032e-01,  ...,  5.3818e-01,\n            5.0019e-01,  5.8124e-01],\n          [ 5.7871e-01,  5.7871e-01,  5.1032e-01,  ...,  5.3818e-01,\n            5.0019e-01,  5.8124e-01],\n          [ 7.1294e-01,  7.1294e-01,  2.3933e-01,  ...,  5.4578e-01,\n            5.5085e-01,  5.5085e-01],\n          ...,\n          [ 2.5706e-01,  2.5706e-01,  2.9758e-01,  ...,  9.2822e-01,\n            6.0403e-01,  3.4181e-02],\n          [ 2.2160e-01,  2.2160e-01,  3.6343e-01,  ...,  1.0042e+00,\n            1.4309e-01,  3.7884e-03],\n          [ 4.9260e-01,  4.9260e-01,  4.4954e-01,  ...,  7.8892e-01,\n            1.0510e-01,  4.6844e-02]],\n \n         ...,\n \n         [[ 1.0712e+00,  1.0712e+00,  9.4257e-01,  ...,  1.3006e+00,\n            1.3046e+00,  1.3106e+00],\n          [ 1.0712e+00,  1.0712e+00,  9.4257e-01,  ...,  1.3006e+00,\n            1.3046e+00,  1.3106e+00],\n          [ 9.6950e-01,  9.6950e-01,  8.3884e-01,  ...,  1.3016e+00,\n            1.3295e+00,  1.3744e+00],\n          ...,\n          [ 1.3964e+00,  1.3964e+00,  1.4163e+00,  ...,  9.9942e-01,\n            8.9669e-01,  7.6704e-01],\n          [ 1.3974e+00,  1.3974e+00,  1.4083e+00,  ...,  9.9543e-01,\n            8.7375e-01,  7.3412e-01],\n          [ 1.4173e+00,  1.4173e+00,  1.4123e+00,  ...,  1.0014e+00,\n            8.7275e-01,  7.3911e-01]],\n \n         [[ 7.6340e-01,  7.6340e-01,  5.9370e-01,  ...,  1.1699e+00,\n            1.1646e+00,  1.1620e+00],\n          [ 7.6340e-01,  7.6340e-01,  5.9370e-01,  ...,  1.1699e+00,\n            1.1646e+00,  1.1620e+00],\n          [ 6.6474e-01,  6.6474e-01,  4.8978e-01,  ...,  1.1778e+00,\n            1.1988e+00,  1.2357e+00],\n          ...,\n          [ 1.0989e+00,  1.0989e+00,  1.1357e+00,  ...,  1.0554e+00,\n            9.2915e-01,  7.9497e-01],\n          [ 1.0844e+00,  1.0844e+00,  1.1225e+00,  ...,  1.0528e+00,\n            9.0284e-01,  7.5288e-01],\n          [ 1.0910e+00,  1.0910e+00,  1.1094e+00,  ...,  1.0554e+00,\n            8.9627e-01,  7.4762e-01]],\n \n         [[ 3.2261e-01,  3.2261e-01,  1.7907e-01,  ...,  3.3639e-01,\n            3.5667e-01,  3.9883e-01],\n          [ 3.2261e-01,  3.2261e-01,  1.7907e-01,  ...,  3.3639e-01,\n            3.5667e-01,  3.9883e-01],\n          [ 2.3827e-01,  2.3827e-01,  1.0528e-01,  ...,  3.3477e-01,\n            3.5018e-01,  3.9235e-01],\n          ...,\n          [ 2.2043e-01,  2.2043e-01,  2.1800e-01,  ...,  4.0127e-01,\n            3.4450e-01,  2.9666e-01],\n          [ 2.0340e-01,  2.0340e-01,  2.0908e-01,  ...,  3.8910e-01,\n            3.1125e-01,  2.4800e-01],\n          [ 1.9286e-01,  1.9286e-01,  1.9610e-01,  ...,  3.8667e-01,\n            2.9990e-01,  2.3178e-01]]], dtype=torch.float64),\n 6)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"ResNet\",\n",
    "    \"ResNet18_Weights\",\n",
    "    \"resnet18\"\n",
    "]\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        #_log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(12, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "model = ResNet(block=Bottleneck, layers=[2, 2, 2, 2])\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 27 15:11:48 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   49C    P0    21W /  N/A |   7092MiB /  7982MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1854      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A      3763      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A     30075      C   ...SS22_AIML/venv/bin/python     7079MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "nll_loss = nll_loss.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(params = model.parameters(), lr = learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "mini_batch_size = 1200\n",
    "train_loader = DataLoader(trainData, batch_size=mini_batch_size, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataloader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1353.6840, 1116.9715, 1041.5042,  945.7788, 1198.5204, 2002.4349,\n         2373.4563, 2300.6978,  732.0590, 1818.9174, 1116.6823, 2599.1740],\n        dtype=torch.float64),\n tensor([ 245.3512,  333.1604,  394.8386,  594.0629,  566.8239,  861.8696,\n         1088.1853, 1119.6170,  404.3341, 1002.6676,  760.1735, 1233.1597],\n        dtype=torch.float64))"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_and_std(train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20220427-13:13:40] epoch: 0 train-loss: 0.9218342796615933\n",
      "[LOG 20220427-13:15:21] epoch: 1 train-loss: 0.3091114001429599\n",
      "[LOG 20220427-13:17:01] epoch: 2 train-loss: 0.20817353803178537\n",
      "[LOG 20220427-13:19:04] epoch: 3 train-loss: 0.15020037442445755\n",
      "[LOG 20220427-13:20:43] epoch: 4 train-loss: 0.1077664500993231\n",
      "[LOG 20220427-13:22:21] epoch: 5 train-loss: 0.09597154528550479\n",
      "[LOG 20220427-13:23:58] epoch: 6 train-loss: 0.06944195715629536\n",
      "[LOG 20220427-13:25:33] epoch: 7 train-loss: 0.06095953982161439\n",
      "[LOG 20220427-13:27:09] epoch: 8 train-loss: 0.050365067208590714\n",
      "[LOG 20220427-13:28:43] epoch: 9 train-loss: 0.039880716687311295\n",
      "[LOG 20220427-13:30:18] epoch: 10 train-loss: 0.050458902169180954\n",
      "[LOG 20220427-13:31:53] epoch: 11 train-loss: 0.04860697127878666\n",
      "[LOG 20220427-13:33:31] epoch: 12 train-loss: 0.03612947650253773\n",
      "[LOG 20220427-13:35:07] epoch: 13 train-loss: 0.033374828569914985\n",
      "[LOG 20220427-13:36:41] epoch: 14 train-loss: 0.03003379147823738\n",
      "[LOG 20220427-13:38:15] epoch: 15 train-loss: 0.025001495590676433\n",
      "[LOG 20220427-13:39:50] epoch: 16 train-loss: 0.02472882142857365\n",
      "[LOG 20220427-13:41:22] epoch: 17 train-loss: 0.03506012864248908\n",
      "[LOG 20220427-13:42:54] epoch: 18 train-loss: 0.02369708017162655\n",
      "[LOG 20220427-13:44:24] epoch: 19 train-loss: 0.011682051476900992\n",
      "[LOG 20220427-13:45:56] epoch: 20 train-loss: 0.010921759490409622\n",
      "[LOG 20220427-13:47:28] epoch: 21 train-loss: 0.009280529915881545\n",
      "[LOG 20220427-13:48:58] epoch: 22 train-loss: 0.015110612320511238\n",
      "[LOG 20220427-13:50:29] epoch: 23 train-loss: 0.022223357009984877\n",
      "[LOG 20220427-13:52:01] epoch: 24 train-loss: 0.018798810439994155\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# start monitoring\n",
    "#wandb.init()\n",
    "\n",
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "model.train()\n",
    "\n",
    "# train the CIFAR10 model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "\n",
    "    # iterate over all-mini batches\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # push mini-batch data to computation device\n",
    "        images = images.to(device, dtype = torch.float)\n",
    "        labels = labels.to(device, dtype = torch.float)\n",
    "\n",
    "        # run forward pass through the network\n",
    "        output = model(images)\n",
    "\n",
    "        # reset graph gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        labels=labels.to(torch.int64)\n",
    "\n",
    "        # determine classification loss\n",
    "        loss = nll_loss(output, labels)\n",
    "\n",
    "        # run backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update network paramaters\n",
    "        optimizer.step()\n",
    "\n",
    "        # collect mini-batch reconstruction loss\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "\n",
    "    # print epoch loss\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "\n",
    "    # set filename of actual model\n",
    "    model_name = 'resnet_bottleneck_model_epoch_{}.pth'.format(str(epoch))\n",
    "\n",
    "    # save current model to GDrive models directory\n",
    "    if (epoch % 10) == 0 or epoch == (num_epochs - 1):\n",
    "      torch.save(model.state_dict(), os.path.join(models_directory, model_name))\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Classification Error $L^{NLL}$', fontsize=10);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(test_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testSamples = glob.glob(os.path.join(test_directory, \"*.npy\"))\n",
    "len(testSamples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, test_directory, transform=False):\n",
    "        self.files = glob.glob(os.path.join(test_directory, \"*.npy\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.files[idx]\n",
    "        image = np.load(item).astype(int)\n",
    "        number = int(item.split('/')[-1].split('_')[1].split('.')[0])\n",
    "\n",
    "        if self.transform:\n",
    "          image = transforms.ToTensor()(image).to(torch.float)\n",
    "\n",
    "        return image, number"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testData = testDataset(test_directory = test_directory, transform = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testData, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# restore pre-trained model snapshot\n",
    "best_model_name = os.path.join(models_directory, 'cifar10_model_epoch_200.pth')\n",
    "\n",
    "# load state_dict from path\n",
    "state_dict_best = torch.load(best_model_name, map_location=torch.device('cpu'))\n",
    "\n",
    "# init pre-trained model class\n",
    "best_model = RESNET18()\n",
    "\n",
    "# load pre-trained models\n",
    "best_model.load_state_dict(state_dict_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "best_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = []\n",
    "numbers = []\n",
    "\n",
    "for i, (images, nums) in enumerate(test_loader):\n",
    "    # run forward pass through the network\n",
    "    pred = torch.argmax(best_model(images), dim=1)\n",
    "    predictions.append(pred.int().item())\n",
    "    numbers.append(nums.int().item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predClasses = np.vectorize(idx_to_class.get)(predictions)\n",
    "\n",
    "d = {'test_id': numbers, 'label': predClasses}\n",
    "predData = pd.DataFrame(data = d)\n",
    "predData = predData.sort_values(by=['test_id'])\n",
    "print(predData.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(predData))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predData.to_csv(os.path.join(models_directory,'submission.csv'), index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx_to_class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}